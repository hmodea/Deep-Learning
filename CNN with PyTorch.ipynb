{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train and test a CNN network on MNIST data using PyTorch framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST dataset\n",
    "\n",
    "x_data=pd.read_csv(\"mnist.csv\",delim_whitespace=True,header=None)\n",
    "y_data=pd.read_csv(\"mnist-labels.csv\",delim_whitespace=True,header=None)\n",
    "\n",
    "train_end=int(x_data.shape[0]*0.5)\n",
    "valid_end=int(train_end+(x_data.shape[0]*0.25))\n",
    "\n",
    "#split into train,validation and test sets (50%,25%,25%)\n",
    "\n",
    "x_train=x_data.iloc[0:train_end,:].values\n",
    "y_train=y_data.iloc[0:train_end,:].values\n",
    "\n",
    "\n",
    "\n",
    "x_valid=x_data.iloc[train_end:valid_end,:].values\n",
    "y_valid=y_data.iloc[train_end:valid_end,:].values\n",
    "\n",
    "x_test=x_data.iloc[valid_end:,:].values\n",
    "y_test=y_data.iloc[valid_end:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to torch format\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_train=x_train.float()\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_train=y_train.float()\n",
    "\n",
    "x_valid=torch.from_numpy(x_valid)\n",
    "x_valid=x_valid.float()\n",
    "y_valid=torch.from_numpy(y_valid)\n",
    "y_valid=y_valid.float()\n",
    "\n",
    "x_test=torch.from_numpy(x_test)\n",
    "x_test=x_test.float()\n",
    "y_test=torch.from_numpy(y_test)\n",
    "y_test=y_test.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.view(-1,1,28,28)\n",
    "x_valid=x_valid.view(-1,1,28,28)\n",
    "x_test=x_test.view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing our convolutional network with 2 conv layer, 2 pooling layers and an output fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,3,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(6,16,3,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(7*7*16,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 7 * 7)\n",
    "        x = F.softmax(self.fc1(x),dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A padding of 1 is chosen with a filter size of 3 to make the pooling layers only responsible for the subsampling of the image.\n",
    "\n",
    "P=(Fâˆ’1)/2 , where p=padding and F=filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(o,y):\n",
    "    \n",
    "    eta=10**-10\n",
    "    loss=-torch.sum(torch.mul(y,torch.log(eta+o)))/y.shape[0]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(o,y):\n",
    "    \n",
    "    c=(o.argmax(1)==y.argmax(1)).double().sum()\n",
    "    return(c.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x,y):\n",
    "    \n",
    "    a=x.view(x.shape[0],-1)\n",
    "    z=torch.cat((a,y),dim=1)\n",
    "    z=z[torch.randperm(a.shape[0]),:]\n",
    "    a=z[:,:z.shape[1]-y.shape[1]]\n",
    "    y=z[:,z.shape[1]-y.shape[1]:]\n",
    "    x=a.view(x.shape[0],x.shape[1],x.shape[2],x.shape[3])\n",
    "    \n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_MiniBatch(x_train,y_train,x_valid,y_valid,epochs,lr,batch_size,mom):\n",
    "    \n",
    "    loss_train=[]\n",
    "    loss_valid=[]\n",
    "    acc_train=[]\n",
    "    acc_valid=[]\n",
    "    batch_train=batch_size\n",
    "    batch_valid=batch_size\n",
    "    \n",
    "    n_valid_batches=np.ceil(x_valid.shape[0]/batch_size)\n",
    "    \n",
    "    correct_train=0\n",
    "    correct_valid=0\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr,momentum=mom)\n",
    "    \n",
    "    p=0 #patience timer for early stopping\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        l_valid=0 #intialize epoch validation loss to zero\n",
    "        \n",
    "        for j in range(0,x_train.shape[0],batch_size):\n",
    "            \n",
    "            \n",
    "            if (x_train.shape[0]-j< batch_size): #cater for the final batch size if #samples not divisible by batch size\n",
    "                \n",
    "                batch_train=x_train.shape[0]-j\n",
    "                \n",
    "            x_train_batch=x_train[j:j+batch_train,:,:,:]\n",
    "            y_train_batch=y_train[j:j+batch_train,:]\n",
    "            \n",
    "            o_train_batch=model(x_train_batch)\n",
    "            l_train=CrossEntropyLoss(o_train_batch,y_train_batch)\n",
    "            correct_train+=correct(o_train_batch,y_train_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        for k in range(0,x_valid.shape[0],batch_size):\n",
    "            \n",
    "            if (x_valid.shape[0]-k< batch_size):#cater for the final batch size if #samples not divisible by batch size\n",
    "                \n",
    "                batch_valid=x_valid.shape[0]-k\n",
    "                    \n",
    "            x_valid_batch=x_valid[k:k+batch_valid,:,:,:]\n",
    "            y_valid_batch=y_valid[k:k+batch_valid,:]\n",
    "                    \n",
    "            o_valid_batch=model(x_valid_batch)\n",
    "            l_valid+=CrossEntropyLoss(o_valid_batch,y_valid_batch)\n",
    "            correct_valid+=correct(o_valid_batch,y_valid_batch)\n",
    "              \n",
    "        l_valid/=n_valid_batches #consider the average validation loss overall batches\n",
    "        \n",
    "        batch_train=batch_size\n",
    "        batch_valid=batch_size\n",
    "        \n",
    "        loss_train.append(l_train)\n",
    "        loss_valid.append(l_valid)\n",
    "        acc_train.append((correct_train*100.0)/x_train.shape[0])\n",
    "        acc_valid.append((correct_valid*100.0)/x_valid.shape[0])\n",
    "        \n",
    "        correct_train=0\n",
    "        correct_valid=0\n",
    "        \n",
    "        print(\"epoch\",i+1)\n",
    "        print(\"training loss\",loss_train[i].item())\n",
    "        print(\"validation loss\",loss_valid[i].item())\n",
    "        print(\"training accuracy\",acc_train[i])\n",
    "        print(\"validation accuracy\",acc_valid[i])\n",
    "        print('########################################################')\n",
    "        \n",
    "        if (i>0):\n",
    "            if(loss_valid[i] >= loss_valid[i-1]):\n",
    "                if(p==10):\n",
    "                    break\n",
    "                else:\n",
    "                    p+=1\n",
    "            else:\n",
    "                p=0\n",
    "            \n",
    "        #shuffle data after each epoch to avoid having same batches\n",
    "        \n",
    "        x_train,y_train=shuffle(x_train,y_train)\n",
    "        x_valid,y_valid=shuffle(x_valid,y_valid)\n",
    "                \n",
    "    return [acc_train,acc_valid,loss_train,loss_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_MiniBatch(x_test,y_test,batch_size):\n",
    "    \n",
    "    batch_test=batch_size\n",
    "    correct_test=0\n",
    "    \n",
    "    for i in range(0,x_test.shape[0],batch_size):\n",
    "        \n",
    "        if (x_test.shape[0]-i< batch_size):\n",
    "            \n",
    "            batch_test=x_test.shape[0]-i\n",
    "                    \n",
    "        x_test_batch=x_test[i:i+batch_test,:,:,:]\n",
    "        y_test_batch=y_test[i:i+batch_test,:]\n",
    "        \n",
    "        o_test_batch=model(x_test_batch)\n",
    "        correct_test+=correct(o_test_batch,y_test_batch)\n",
    "        \n",
    "    return (correct_test*100.0)/x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "epochs=200\n",
    "lr=0.1\n",
    "batch_size=128\n",
    "mom=0.5\n",
    "\n",
    "model=ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "training loss 0.04637962952256203\n",
      "validation loss 0.18714271485805511\n",
      "training accuracy 84.56\n",
      "validation accuracy 74.9\n",
      "########################################################\n",
      "epoch 2\n",
      "training loss 0.04356582090258598\n",
      "validation loss 0.3154463469982147\n",
      "training accuracy 96.22666666666667\n",
      "validation accuracy 95.71333333333334\n",
      "########################################################\n",
      "epoch 3\n",
      "training loss 0.07054323703050613\n",
      "validation loss 0.01927008293569088\n",
      "training accuracy 97.19\n",
      "validation accuracy 96.76666666666667\n",
      "########################################################\n",
      "epoch 4\n",
      "training loss 0.02667125128209591\n",
      "validation loss 0.019634271040558815\n",
      "training accuracy 97.59333333333333\n",
      "validation accuracy 97.17333333333333\n",
      "########################################################\n",
      "epoch 5\n",
      "training loss 0.10852930694818497\n",
      "validation loss 0.029674524441361427\n",
      "training accuracy 97.82\n",
      "validation accuracy 97.3\n",
      "########################################################\n",
      "epoch 6\n",
      "training loss 0.02971295826137066\n",
      "validation loss 0.20519323647022247\n",
      "training accuracy 98.14\n",
      "validation accuracy 97.46666666666667\n",
      "########################################################\n",
      "epoch 7\n",
      "training loss 0.026429379358887672\n",
      "validation loss 0.06869503855705261\n",
      "training accuracy 98.24666666666667\n",
      "validation accuracy 97.80666666666667\n",
      "########################################################\n",
      "epoch 8\n",
      "training loss 0.1040179654955864\n",
      "validation loss 0.03501104190945625\n",
      "training accuracy 98.34333333333333\n",
      "validation accuracy 97.72\n",
      "########################################################\n",
      "epoch 9\n",
      "training loss 0.056107550859451294\n",
      "validation loss 0.08901997655630112\n",
      "training accuracy 98.46\n",
      "validation accuracy 97.8\n",
      "########################################################\n",
      "epoch 10\n",
      "training loss 0.005189842544496059\n",
      "validation loss 0.4006723463535309\n",
      "training accuracy 98.60666666666667\n",
      "validation accuracy 97.85333333333334\n",
      "########################################################\n",
      "epoch 11\n",
      "training loss 0.008263944648206234\n",
      "validation loss 0.11411726474761963\n",
      "training accuracy 98.64\n",
      "validation accuracy 97.87333333333333\n",
      "########################################################\n",
      "epoch 12\n",
      "training loss 0.08846598118543625\n",
      "validation loss 0.06826788932085037\n",
      "training accuracy 98.75\n",
      "validation accuracy 97.90666666666667\n",
      "########################################################\n",
      "epoch 13\n",
      "training loss 0.03770403191447258\n",
      "validation loss 0.0015261598164215684\n",
      "training accuracy 98.76666666666667\n",
      "validation accuracy 97.9\n",
      "########################################################\n",
      "epoch 14\n",
      "training loss 0.03938398137688637\n",
      "validation loss 0.0037416350096464157\n",
      "training accuracy 98.92333333333333\n",
      "validation accuracy 98.04666666666667\n",
      "########################################################\n",
      "epoch 15\n",
      "training loss 0.026102200150489807\n",
      "validation loss 0.08912179619073868\n",
      "training accuracy 99.00666666666666\n",
      "validation accuracy 98.02\n",
      "########################################################\n",
      "epoch 16\n",
      "training loss 0.025803184136748314\n",
      "validation loss 0.0009696578490547836\n",
      "training accuracy 98.96333333333334\n",
      "validation accuracy 97.92666666666666\n",
      "########################################################\n",
      "epoch 17\n",
      "training loss 0.07325894385576248\n",
      "validation loss 0.04463392123579979\n",
      "training accuracy 99.03\n",
      "validation accuracy 97.97333333333333\n",
      "########################################################\n",
      "epoch 18\n",
      "training loss 0.007469137664884329\n",
      "validation loss 0.05104967951774597\n",
      "training accuracy 99.14\n",
      "validation accuracy 98.02\n",
      "########################################################\n",
      "epoch 19\n",
      "training loss 0.06632141023874283\n",
      "validation loss 0.2418065220117569\n",
      "training accuracy 99.11666666666666\n",
      "validation accuracy 98.03333333333333\n",
      "########################################################\n",
      "epoch 20\n",
      "training loss 0.0011711850529536605\n",
      "validation loss 0.009920171461999416\n",
      "training accuracy 99.26666666666667\n",
      "validation accuracy 97.95333333333333\n",
      "########################################################\n",
      "epoch 21\n",
      "training loss 0.024309543892741203\n",
      "validation loss 0.014509093947708607\n",
      "training accuracy 99.23666666666666\n",
      "validation accuracy 98.03333333333333\n",
      "########################################################\n",
      "epoch 22\n",
      "training loss 0.0008338710758835077\n",
      "validation loss 0.036018189042806625\n",
      "training accuracy 99.18\n",
      "validation accuracy 98.07333333333334\n",
      "########################################################\n",
      "epoch 23\n",
      "training loss 0.0020963200367987156\n",
      "validation loss 0.04735463485121727\n",
      "training accuracy 99.28666666666666\n",
      "validation accuracy 98.18\n",
      "########################################################\n",
      "epoch 24\n",
      "training loss 0.010691412724554539\n",
      "validation loss 0.02634146623313427\n",
      "training accuracy 99.33333333333333\n",
      "validation accuracy 98.00666666666666\n",
      "########################################################\n",
      "epoch 25\n",
      "training loss 0.004475766327232122\n",
      "validation loss 0.012339937500655651\n",
      "training accuracy 99.38\n",
      "validation accuracy 98.14666666666666\n",
      "########################################################\n",
      "epoch 26\n",
      "training loss 0.017306460067629814\n",
      "validation loss 0.0011532403295859694\n",
      "training accuracy 99.46666666666667\n",
      "validation accuracy 98.15333333333334\n",
      "########################################################\n",
      "epoch 27\n",
      "training loss 0.002489276695996523\n",
      "validation loss 0.09006751328706741\n",
      "training accuracy 99.43333333333334\n",
      "validation accuracy 98.12666666666667\n",
      "########################################################\n",
      "epoch 28\n",
      "training loss 0.0007610351894982159\n",
      "validation loss 0.06406789273023605\n",
      "training accuracy 99.55666666666667\n",
      "validation accuracy 98.11333333333333\n",
      "########################################################\n",
      "epoch 29\n",
      "training loss 0.0022451800759881735\n",
      "validation loss 0.0001651501515880227\n",
      "training accuracy 99.52666666666667\n",
      "validation accuracy 97.85333333333334\n",
      "########################################################\n",
      "epoch 30\n",
      "training loss 0.019175561144948006\n",
      "validation loss 0.003230838803574443\n",
      "training accuracy 99.53666666666666\n",
      "validation accuracy 98.1\n",
      "########################################################\n",
      "epoch 31\n",
      "training loss 0.0035594524815678596\n",
      "validation loss 0.012038958258926868\n",
      "training accuracy 99.55333333333333\n",
      "validation accuracy 98.17333333333333\n",
      "########################################################\n",
      "epoch 32\n",
      "training loss 0.0145824970677495\n",
      "validation loss 0.10954689234495163\n",
      "training accuracy 99.56333333333333\n",
      "validation accuracy 98.06666666666666\n",
      "########################################################\n",
      "epoch 33\n",
      "training loss 0.022402716800570488\n",
      "validation loss 0.056514058262109756\n",
      "training accuracy 99.52666666666667\n",
      "validation accuracy 98.12\n",
      "########################################################\n",
      "epoch 34\n",
      "training loss 0.0023444299586117268\n",
      "validation loss 0.004175378475338221\n",
      "training accuracy 99.60666666666667\n",
      "validation accuracy 98.13333333333334\n",
      "########################################################\n",
      "epoch 35\n",
      "training loss 0.0021073620300740004\n",
      "validation loss 0.1623375564813614\n",
      "training accuracy 99.64333333333333\n",
      "validation accuracy 98.12666666666667\n",
      "########################################################\n",
      "epoch 36\n",
      "training loss 0.007130607962608337\n",
      "validation loss 0.04986311122775078\n",
      "training accuracy 99.71666666666667\n",
      "validation accuracy 98.14\n",
      "########################################################\n",
      "epoch 37\n",
      "training loss 0.013813906349241734\n",
      "validation loss 0.2731300890445709\n",
      "training accuracy 99.75333333333333\n",
      "validation accuracy 98.13333333333334\n",
      "########################################################\n",
      "epoch 38\n",
      "training loss 0.002476439578458667\n",
      "validation loss 0.0001584522979101166\n",
      "training accuracy 99.83\n",
      "validation accuracy 98.16\n",
      "########################################################\n",
      "epoch 39\n",
      "training loss 0.0018559126183390617\n",
      "validation loss 4.0786868339637294e-05\n",
      "training accuracy 99.79\n",
      "validation accuracy 98.12\n",
      "########################################################\n",
      "epoch 40\n",
      "training loss 0.0010280897840857506\n",
      "validation loss 0.029604412615299225\n",
      "training accuracy 99.76\n",
      "validation accuracy 98.12666666666667\n",
      "########################################################\n",
      "epoch 41\n",
      "training loss 0.0017665046034380794\n",
      "validation loss 0.0006056184647604823\n",
      "training accuracy 99.71333333333334\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42\n",
      "training loss 0.0008717275341041386\n",
      "validation loss 0.3377530574798584\n",
      "training accuracy 99.84333333333333\n",
      "validation accuracy 98.10666666666667\n",
      "########################################################\n",
      "epoch 43\n",
      "training loss 0.0008820178918540478\n",
      "validation loss 0.19723105430603027\n",
      "training accuracy 99.78333333333333\n",
      "validation accuracy 98.18666666666667\n",
      "########################################################\n",
      "epoch 44\n",
      "training loss 0.006510432343930006\n",
      "validation loss 0.24893033504486084\n",
      "training accuracy 99.81666666666666\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 45\n",
      "training loss 0.0034120017662644386\n",
      "validation loss 0.0012613849248737097\n",
      "training accuracy 99.88333333333334\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 46\n",
      "training loss 0.0005130220088176429\n",
      "validation loss 0.00014539230323862284\n",
      "training accuracy 99.8\n",
      "validation accuracy 98.16666666666667\n",
      "########################################################\n",
      "epoch 47\n",
      "training loss 0.0010596363572403789\n",
      "validation loss 0.05380725860595703\n",
      "training accuracy 99.88333333333334\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 48\n",
      "training loss 0.0026823924854397774\n",
      "validation loss 0.005660690367221832\n",
      "training accuracy 99.98333333333333\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 49\n",
      "training loss 0.0002819153305608779\n",
      "validation loss 0.1466100960969925\n",
      "training accuracy 99.97\n",
      "validation accuracy 98.3\n",
      "########################################################\n",
      "epoch 50\n",
      "training loss 0.031449463218450546\n",
      "validation loss 0.001862741424702108\n",
      "training accuracy 99.95\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 51\n",
      "training loss 0.0015905541367828846\n",
      "validation loss 0.0021822284907102585\n",
      "training accuracy 99.89\n",
      "validation accuracy 98.16\n",
      "########################################################\n",
      "epoch 52\n",
      "training loss 0.0009560442413203418\n",
      "validation loss 0.012839287519454956\n",
      "training accuracy 99.92333333333333\n",
      "validation accuracy 98.19333333333333\n",
      "########################################################\n",
      "epoch 53\n",
      "training loss 0.0026887680869549513\n",
      "validation loss 0.16065163910388947\n",
      "training accuracy 99.89\n",
      "validation accuracy 98.06666666666666\n",
      "########################################################\n",
      "epoch 54\n",
      "training loss 0.0008983339066617191\n",
      "validation loss 1.7906390894495416e-06\n",
      "training accuracy 99.98333333333333\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 55\n",
      "training loss 8.682182669872418e-05\n",
      "validation loss 4.1226653024750703e-07\n",
      "training accuracy 99.95666666666666\n",
      "validation accuracy 98.16\n",
      "########################################################\n",
      "epoch 56\n",
      "training loss 0.0003920209128409624\n",
      "validation loss 0.004119656514376402\n",
      "training accuracy 99.98666666666666\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 57\n",
      "training loss 0.00023234709806274623\n",
      "validation loss 0.42433473467826843\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.28666666666666\n",
      "########################################################\n",
      "epoch 58\n",
      "training loss 0.00014210208610165864\n",
      "validation loss 0.25349852442741394\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.28666666666666\n",
      "########################################################\n",
      "epoch 59\n",
      "training loss 0.00012982057523913682\n",
      "validation loss 0.06175151467323303\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 60\n",
      "training loss 0.00075634935637936\n",
      "validation loss 0.00023139359836932272\n",
      "training accuracy 99.99666666666667\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 61\n",
      "training loss 5.918153328821063e-05\n",
      "validation loss 1.1051765795855317e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 62\n",
      "training loss 0.00045160308945924044\n",
      "validation loss 0.012111484073102474\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.28\n",
      "########################################################\n",
      "epoch 63\n",
      "training loss 0.0012080008164048195\n",
      "validation loss 0.19041620194911957\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 64\n",
      "training loss 6.944800134078832e-06\n",
      "validation loss 0.001985483104363084\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.29333333333334\n",
      "########################################################\n",
      "epoch 65\n",
      "training loss 0.0010060289641842246\n",
      "validation loss 0.0002634211559779942\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 66\n",
      "training loss 4.9443202442489564e-05\n",
      "validation loss 0.007526864763349295\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 67\n",
      "training loss 0.0008464169222861528\n",
      "validation loss 0.22456105053424835\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 68\n",
      "training loss 7.446739618899301e-05\n",
      "validation loss 0.1506020426750183\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 69\n",
      "training loss 4.455739144759718e-06\n",
      "validation loss 0.37374866008758545\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 70\n",
      "training loss 0.0010262400610372424\n",
      "validation loss 2.2848486480597785e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 71\n",
      "training loss 0.0003483888285700232\n",
      "validation loss 0.00483058113604784\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 72\n",
      "training loss 0.0006998279131948948\n",
      "validation loss 0.17515218257904053\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.28666666666666\n",
      "########################################################\n",
      "epoch 73\n",
      "training loss 0.0005022114492021501\n",
      "validation loss 0.14431129395961761\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 74\n",
      "training loss 4.186284786555916e-05\n",
      "validation loss 2.8967688194825314e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 75\n",
      "training loss 4.7425262891920283e-05\n",
      "validation loss 4.693892208251782e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 76\n",
      "training loss 0.00028416866553016007\n",
      "validation loss 4.967053879312289e-09\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 77\n",
      "training loss 0.00027917171246372163\n",
      "validation loss 3.0249796054704348e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 78\n",
      "training loss 0.000122425306471996\n",
      "validation loss 0.00392526388168335\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 79\n",
      "training loss 8.17058844404528e-06\n",
      "validation loss 0.5482816100120544\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.28\n",
      "########################################################\n",
      "epoch 80\n",
      "training loss 1.2852437976107467e-06\n",
      "validation loss 0.3866780996322632\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 81\n",
      "training loss 0.00023224162578117102\n",
      "validation loss 0.0001513642055215314\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 82\n",
      "training loss 0.0001173214113805443\n",
      "validation loss 0.0009149848483502865\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83\n",
      "training loss 0.00010619778913678601\n",
      "validation loss 0.00037595434696413577\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 84\n",
      "training loss 1.0555015705904225e-07\n",
      "validation loss 0.004795608576387167\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 85\n",
      "training loss 0.0006975780124776065\n",
      "validation loss 0.0010602085385471582\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 86\n",
      "training loss 0.001386273535899818\n",
      "validation loss 0.002935644006356597\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 87\n",
      "training loss 5.690938633051701e-05\n",
      "validation loss 0.0515146441757679\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 88\n",
      "training loss 1.6499607227160595e-05\n",
      "validation loss 0.0022296085953712463\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 89\n",
      "training loss 0.0008257497102022171\n",
      "validation loss 4.044484376208857e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 90\n",
      "training loss 6.457676499849185e-05\n",
      "validation loss 0.19584853947162628\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 91\n",
      "training loss 1.8498607460060157e-05\n",
      "validation loss 0.00033181640901602805\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 92\n",
      "training loss 0.00010401528561487794\n",
      "validation loss 9.880296602204908e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 93\n",
      "training loss 0.00022946270473767072\n",
      "validation loss 0.43569302558898926\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.27333333333333\n",
      "########################################################\n",
      "epoch 94\n",
      "training loss 0.0003318963572382927\n",
      "validation loss 0.04230774566531181\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 95\n",
      "training loss 0.0004614413483068347\n",
      "validation loss 0.0061171031557023525\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 96\n",
      "training loss 0.0012746906140819192\n",
      "validation loss 0.9459972381591797\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 97\n",
      "training loss 0.00010846442455658689\n",
      "validation loss 0.056070033460855484\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 98\n",
      "training loss 0.0004501538642216474\n",
      "validation loss 0.07197672873735428\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 99\n",
      "training loss 3.3120595617219806e-05\n",
      "validation loss 4.718703294770421e-08\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 100\n",
      "training loss 0.0003099263703916222\n",
      "validation loss 1.079976118489867e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.25333333333333\n",
      "########################################################\n",
      "epoch 101\n",
      "training loss 1.981899004022125e-06\n",
      "validation loss 0.0007911510183475912\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 102\n",
      "training loss 3.0460550988209434e-05\n",
      "validation loss 0.11943674087524414\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 103\n",
      "training loss 7.420795009238645e-05\n",
      "validation loss 6.28549896646291e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 104\n",
      "training loss 0.00043125811498612165\n",
      "validation loss 0.6356813311576843\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 105\n",
      "training loss 0.0002488756726961583\n",
      "validation loss 0.03501713648438454\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 106\n",
      "training loss 0.00012987697846256196\n",
      "validation loss 1.7132404536823742e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 107\n",
      "training loss 2.227244658570271e-05\n",
      "validation loss 0.0004146033897995949\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 108\n",
      "training loss 2.2918256945558824e-05\n",
      "validation loss 0.0022288283798843622\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 109\n",
      "training loss 8.456650903099217e-06\n",
      "validation loss 0.1549074500799179\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26666666666667\n",
      "########################################################\n",
      "epoch 110\n",
      "training loss 0.0005289975670166314\n",
      "validation loss 9.064947903425491e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 111\n",
      "training loss 2.829921868396923e-05\n",
      "validation loss 0.3201369345188141\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 112\n",
      "training loss 4.3659561924869195e-05\n",
      "validation loss 0.01453727949410677\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 113\n",
      "training loss 0.00022336268739309162\n",
      "validation loss 0.5476022958755493\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24\n",
      "########################################################\n",
      "epoch 114\n",
      "training loss 3.5266370446151996e-07\n",
      "validation loss 0.0048249331302940845\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 115\n",
      "training loss 8.263313793577254e-05\n",
      "validation loss 0.00014054572966415435\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 116\n",
      "training loss 4.497413829085417e-05\n",
      "validation loss 0.011788551695644855\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 117\n",
      "training loss 0.0002625071501825005\n",
      "validation loss 0.002722040517255664\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.26\n",
      "########################################################\n",
      "epoch 118\n",
      "training loss 3.3616717701079324e-05\n",
      "validation loss 4.5699021029577125e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 119\n",
      "training loss 0.00018699899374041706\n",
      "validation loss 0.5567939281463623\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 120\n",
      "training loss 0.00014486537838820368\n",
      "validation loss 2.0861652672010678e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 121\n",
      "training loss 0.00023322874039877206\n",
      "validation loss 5.463760999191436e-08\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 122\n",
      "training loss 0.0005681400652974844\n",
      "validation loss 0.0004170443571638316\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 123\n",
      "training loss 9.972161933546886e-05\n",
      "validation loss 0.003179369494318962\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 124\n",
      "training loss 9.846800821833313e-05\n",
      "validation loss 0.9038240909576416\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 125\n",
      "training loss 0.0002504572330508381\n",
      "validation loss 3.9466478483518586e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 126\n",
      "training loss 1.0542640893618227e-06\n",
      "validation loss 0.005237358156591654\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 127\n",
      "training loss 0.00021958300203550607\n",
      "validation loss 0.07130668312311172\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.25333333333333\n",
      "########################################################\n",
      "epoch 128\n",
      "training loss 0.00014914329221937805\n",
      "validation loss 0.20966213941574097\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 129\n",
      "training loss 2.8146125259809196e-05\n",
      "validation loss 1.74626966327196e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 130\n",
      "training loss 0.00018378387903794646\n",
      "validation loss 0.0003403525333851576\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 131\n",
      "training loss 0.0005091932252980769\n",
      "validation loss 4.9670543234014986e-09\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 132\n",
      "training loss 0.0004970959271304309\n",
      "validation loss 0.023770689964294434\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 133\n",
      "training loss 0.0001297266426263377\n",
      "validation loss 0.016292588785290718\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 134\n",
      "training loss 0.00043488849769346416\n",
      "validation loss 0.009417570196092129\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 135\n",
      "training loss 5.5023894674377516e-05\n",
      "validation loss 0.07143216580152512\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 136\n",
      "training loss 4.796537177753635e-05\n",
      "validation loss 0.35486599802970886\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 137\n",
      "training loss 6.056668280507438e-05\n",
      "validation loss 2.4835273393364332e-08\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 138\n",
      "training loss 0.00013185430725570768\n",
      "validation loss 6.308194997473038e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 139\n",
      "training loss 3.6099334010941675e-06\n",
      "validation loss 0.4377419948577881\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.18666666666667\n",
      "########################################################\n",
      "epoch 140\n",
      "training loss 7.223673037515255e-06\n",
      "validation loss 9.511948633189604e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 141\n",
      "training loss 2.3696169591858052e-05\n",
      "validation loss 0.40750768780708313\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 142\n",
      "training loss 0.00015042941959109157\n",
      "validation loss 1.0083222150569782e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 143\n",
      "training loss 4.962442744727014e-06\n",
      "validation loss 0.00010162041144212708\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 144\n",
      "training loss 8.816525820520837e-08\n",
      "validation loss -0.0\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 145\n",
      "training loss 6.048641444067471e-05\n",
      "validation loss 0.0003572142159100622\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 146\n",
      "training loss 2.4611335902591236e-05\n",
      "validation loss 1.6664647546349443e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 147\n",
      "training loss 6.182937795529142e-05\n",
      "validation loss 1.8605625882628374e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 148\n",
      "training loss 0.00015394009824376553\n",
      "validation loss 0.0002390216541243717\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 149\n",
      "training loss 9.207877155859023e-06\n",
      "validation loss 0.0022460222244262695\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 150\n",
      "training loss 2.3738124582450837e-05\n",
      "validation loss 0.0007106208358891308\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 151\n",
      "training loss 5.33963600446441e-07\n",
      "validation loss 0.001044146134518087\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 152\n",
      "training loss 0.00018839030235540122\n",
      "validation loss 0.0011496118968352675\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22666666666667\n",
      "########################################################\n",
      "epoch 153\n",
      "training loss 1.3509689779311884e-05\n",
      "validation loss 0.43894556164741516\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 154\n",
      "training loss 2.123418170185687e-07\n",
      "validation loss 1.5348474562415504e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.19333333333333\n",
      "########################################################\n",
      "epoch 155\n",
      "training loss 4.9324178689857945e-05\n",
      "validation loss 0.16974401473999023\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 156\n",
      "training loss 0.0001823053607949987\n",
      "validation loss 8.443999632845589e-08\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 157\n",
      "training loss 6.99173760949634e-05\n",
      "validation loss 6.931981715752045e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 158\n",
      "training loss 7.100771199475275e-06\n",
      "validation loss 0.009456110186874866\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 159\n",
      "training loss 3.725290298461914e-09\n",
      "validation loss 0.0539637915790081\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 160\n",
      "training loss 6.51771915727295e-06\n",
      "validation loss 0.0025623657274991274\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 161\n",
      "training loss 0.00026724915369413793\n",
      "validation loss 3.352769226694363e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 162\n",
      "training loss 0.00013960304204374552\n",
      "validation loss 4.182438260613708e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 163\n",
      "training loss 4.912727672490291e-05\n",
      "validation loss 0.32509252429008484\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.23333333333333\n",
      "########################################################\n",
      "epoch 164\n",
      "training loss 4.2141080484725535e-05\n",
      "validation loss 0.00019542356312740594\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 165\n",
      "training loss 1.440450461132059e-07\n",
      "validation loss 0.0005406368873082101\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 166\n",
      "training loss 0.0002706789819058031\n",
      "validation loss 0.10623699426651001\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 167\n",
      "training loss 6.27565459581092e-05\n",
      "validation loss 0.00013884651707485318\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 168\n",
      "training loss 1.6033813153626397e-05\n",
      "validation loss 3.621087671490386e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 169\n",
      "training loss 0.00010603675036691129\n",
      "validation loss 9.934108646802997e-09\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 170\n",
      "training loss 5.776808029622771e-05\n",
      "validation loss 0.103737473487854\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.18666666666667\n",
      "########################################################\n",
      "epoch 171\n",
      "training loss 1.707541014184244e-05\n",
      "validation loss 3.00517581308668e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 172\n",
      "training loss 4.206071935186628e-06\n",
      "validation loss 0.1616828292608261\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 173\n",
      "training loss 8.450735913356766e-05\n",
      "validation loss -0.0\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 174\n",
      "training loss 4.145253842580132e-05\n",
      "validation loss 0.31270381808280945\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 175\n",
      "training loss 0.0002176584821427241\n",
      "validation loss 0.006385632324963808\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 176\n",
      "training loss 6.826120807090774e-05\n",
      "validation loss 0.00016516530013177544\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.24666666666667\n",
      "########################################################\n",
      "epoch 177\n",
      "training loss 8.275095751741901e-05\n",
      "validation loss 0.0026602281723171473\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 178\n",
      "training loss 0.00014250895765144378\n",
      "validation loss 1.1548560223673121e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 179\n",
      "training loss 0.00014654807455372065\n",
      "validation loss 0.06279020756483078\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 180\n",
      "training loss 1.1175890080039608e-07\n",
      "validation loss 0.03760015591979027\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 181\n",
      "training loss 0.00023990283079911023\n",
      "validation loss 1.395522117614746\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.19333333333333\n",
      "########################################################\n",
      "epoch 182\n",
      "training loss 0.00013887917157262564\n",
      "validation loss 1.2417636696682166e-08\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 183\n",
      "training loss 1.95439497474581e-05\n",
      "validation loss 9.934107758624577e-09\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.19333333333333\n",
      "########################################################\n",
      "epoch 184\n",
      "training loss 1.3659399833443331e-08\n",
      "validation loss 0.010527203790843487\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 185\n",
      "training loss 5.389306352299172e-07\n",
      "validation loss 9.766710718395188e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.19333333333333\n",
      "########################################################\n",
      "epoch 186\n",
      "training loss 0.00019072093709837645\n",
      "validation loss 2.5660132450866513e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 187\n",
      "training loss 0.00011106476449640468\n",
      "validation loss 0.021587572991847992\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 188\n",
      "training loss 1.744646579027176e-05\n",
      "validation loss 0.00015882898878771812\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 189\n",
      "training loss 3.211416697013192e-05\n",
      "validation loss 9.465547009313013e-06\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 190\n",
      "training loss 5.596588016487658e-05\n",
      "validation loss 0.00015785406867507845\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 191\n",
      "training loss 7.527143679908477e-06\n",
      "validation loss 2.7944051907979883e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.2\n",
      "########################################################\n",
      "epoch 192\n",
      "training loss 1.594839159224648e-05\n",
      "validation loss 3.9984968225326156e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 193\n",
      "training loss 2.6422780138091184e-05\n",
      "validation loss 0.17096461355686188\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 194\n",
      "training loss 3.3357518987031654e-05\n",
      "validation loss 0.0007521084626205266\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 195\n",
      "training loss 0.00010089108400279656\n",
      "validation loss 3.2534330784983467e-07\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 196\n",
      "training loss 6.544165080413222e-05\n",
      "validation loss 1.4901162970204496e-08\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 197\n",
      "training loss 0.00016850353858899325\n",
      "validation loss 0.0002944951120298356\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.21333333333334\n",
      "########################################################\n",
      "epoch 198\n",
      "training loss 6.154742004582658e-05\n",
      "validation loss 0.041222620755434036\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.22\n",
      "########################################################\n",
      "epoch 199\n",
      "training loss 0.00011518139945110306\n",
      "validation loss 1.9088092813035473e-05\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "epoch 200\n",
      "training loss 6.973487325012684e-05\n",
      "validation loss 0.00015998842718545347\n",
      "training accuracy 100.0\n",
      "validation accuracy 98.20666666666666\n",
      "########################################################\n",
      "training time= 1024.3954702049996\n"
     ]
    }
   ],
   "source": [
    "start_time=timeit.default_timer()\n",
    "acc_train,acc_valid,loss_train,loss_valid=train_NN_MiniBatch(x_train,y_train,\n",
    "                                                             x_valid,y_valid,epochs,lr,batch_size,mom)\n",
    "elapsed_time=timeit.default_timer()-start_time\n",
    "print(\"training time=\",elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Training, validation accuracies')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67d0eb5630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_train)\n",
    "plt.plot(acc_valid)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.title(\"Training, validation accuracies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy= 98.26666666666667\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "\n",
    "test_acc=test_MiniBatch(x_test,y_test,batch_size=128)\n",
    "print(\"Test accuracy=\",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
